{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from csv import DictWriter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold # GroupShuffleSplit, RepeatedStratifiedKFold, PredefinedSplit, StratifiedKFold, LeavePGroupsOut\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.pipeline import Pipeline # Se usa este y no sklearn.pipeline.Pipeline para poder añadir over_sampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE # ADASYN, SMOTENC\n",
    "# from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, cohen_kappa_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, recall_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiers = {\n",
    "        \"SVC\": SVC(C=1, kernel= 'rbf', gamma = 'scale', probability=True),\n",
    "        \"KNN\": KNN(n_neighbors=5, weights='uniform', metric='minkowski'),\n",
    "}\n",
    "dict_parameters = {\n",
    "        \"SVC\": {'SVC__kernel': ['rbf','sigmoid'],\n",
    "                'SVC__gamma': [0.001, 0.01, 0.1, 1, 'auto','scale'],\n",
    "                'SVC__C': [1, 10, 100, 1000],\n",
    "                'SVC__probability': [True]},\n",
    "        \"KNN\": {'KNN__n_neighbors': [1,3,5,7],\n",
    "                'KNN__weights': ['uniform','distance'],\n",
    "                'KNN__metric': ['euclidean','manhattan','minkowski']},\n",
    "        }\n",
    "\n",
    "# dict_resample = {'SMOTE': SMOTE(random_state=11, k_neighbors=4), # si son todo variables numericas, se puede usar SMOTE\n",
    "#                  'RandomOverSampler': RandomOverSampler(sampling_strategy='not majority')}\n",
    "\n",
    "scoring = {'WAcc': 'accuracy',\n",
    "           'UAcc': make_scorer(balanced_accuracy_score),\n",
    "           'kappa': make_scorer(cohen_kappa_score), \n",
    "           'auc':make_scorer(roc_auc_score, multi_class = 'ovr', needs_proba=True),\n",
    "           'f1':make_scorer(f1_score, average = 'weighted'), # metric for each labe -> average weighted by support (the number of true instances for each label)\n",
    "           'precision':make_scorer(precision_score, average = 'weighted', zero_division = 0),\n",
    "           'recall':make_scorer(recall_score, average = 'weighted'), # Sensitivity = recall\n",
    "#            'specificity': make_scorer(recall_score, pos_label=0), \n",
    "          }\n",
    "\n",
    "def transform_dict(dict_params):\n",
    "    new_dict = dict()\n",
    "    for k in dict_params.keys():\n",
    "        new_name = k.split('__')[1]\n",
    "        new_dict[new_name] = dict_params[k]\n",
    "    return new_dict\n",
    "\n",
    "def binarizar(data):\n",
    "    \"\"\" Funcion para pasar de la escala [-2,-1,0,1,2] a la escala [-1,0,1] \"\"\"\n",
    "    new_data = list()\n",
    "    for n in data:\n",
    "        if n == -2:\n",
    "            label = -1\n",
    "        elif n == -1:\n",
    "            label = -1\n",
    "        elif n == 0:\n",
    "            label = 0\n",
    "        elif n == 1:\n",
    "            label = 1\n",
    "        elif n == 2:\n",
    "            label = 1\n",
    "        else:\n",
    "            print('Problemas en',n)\n",
    "        new_data.append(label)\n",
    "    return new_data\n",
    "\n",
    "def load_dataframe(dataset):\n",
    "    # Load features\n",
    "    df_features = pd.read_excel('data/'+dataset+'/features/eGeMAPS_functionals.xlsx')\n",
    "    df_features.rename(columns={\"id\": \"file\"}, inplace=True)\n",
    "    \n",
    "    # Load partitions\n",
    "    test_partition = pd.read_csv('data/'+dataset+'/data_partitions/TrainValTest/split_testing.csv',sep=\"\\t\")\n",
    "    test_partition['split'] = 'test'\n",
    "    validation_partition = pd.read_csv('data/'+dataset+'/data_partitions/TrainValTest/split_validation.csv',sep=\"\\t\")\n",
    "    validation_partition['split'] = 'val'\n",
    "    training_partition = pd.read_csv('data/'+dataset+'/data_partitions/TrainValTest/split_training.csv',sep=\"\\t\")\n",
    "    training_partition['split'] = 'train'\n",
    "    partitions = pd.concat([training_partition, validation_partition, test_partition])\n",
    "    partitions.reset_index(inplace=True, drop = True)\n",
    "    \n",
    "    if dataset == 'VOSOME':\n",
    "        \n",
    "        # Incluir archivos con id negativo (seleccionados para ser evaluados los primeros)\n",
    "        partitions['file'] = np.abs(partitions.file.values) \n",
    "    \n",
    "        # Load labels from psychologists\n",
    "        partitions.rename(columns={'valence':'valence_3r','arousal':'arousal_3r'}, inplace=True)\n",
    "        \n",
    "        df_gold = pd.read_excel('data/VOSOME/labels/labels_Psicologas.xlsx')\n",
    "        df_gold['file'] = df_gold['Filename'].str[0:-4]\n",
    "        df_gold['file'] = df_gold['file'].astype('int')\n",
    "        df_gold['arousal_binarize'] = binarizar(df_gold['Arousal'].values)\n",
    "        df_gold['valencia_binarize'] = binarizar(df_gold['Valencia'].values)\n",
    "\n",
    "        merged_df = partitions.merge(df_gold[['file','valencia_binarize' ,'arousal_binarize','Emotion']], on='file', how='left')\n",
    "        partitions['valence_psy'] = merged_df['valencia_binarize']\n",
    "        partitions['arousal_psy'] = merged_df['arousal_binarize']\n",
    "        partitions['emotion'] = merged_df['Emotion']\n",
    "\n",
    "        # Add new column with only 4 emotions\n",
    "        categories_out = ['Tristeza', 'Temor','Asco']  # List of strings to be replaced\n",
    "        partitions['emotion4'] = partitions['emotion'].apply(lambda x: 10 if x in categories_out else x)\n",
    "\n",
    "        # Merge dataframes\n",
    "        df = pd.merge(partitions, df_features, on=\"file\")\n",
    "\n",
    "        # Add extra labels (acuerdo total evaludores, combinacion raters + evaluadores)\n",
    "        # df_extra = pd.read_excel('data/'+dataset+'/labels/labels_extra.xlsx')\n",
    "        df_extra = pd.read_excel('data/'+dataset+'/labels/labels_extra_v2.xlsx')\n",
    "        new_partitions = df_extra.merge(df, on='file', how='left') \n",
    "    else:\n",
    "        # Merge dataframes\n",
    "        new_partitions = pd.merge(partitions, df_features, on=\"file\")\n",
    "    \n",
    "    return new_partitions\n",
    "\n",
    "def separate_Xy(df, label):\n",
    "    # Nota: los casos a eliminar (label=10) se eliminan antes!\n",
    "    X = df.loc[:,'F0semitoneFrom27.5Hz_sma3nz_amean':].copy() \n",
    "    if label in ['valence_3r', 'valencia_3rAcuerdo', 'valence_psy', 'valencia_combined', 'valencia_combined_v2', \n",
    "                 'arousal_3r', 'arousal_3rAcuerdo', 'arousal_psy', 'arousal_combined',  'arousal_combined_v2',\n",
    "                 'valence','arousal']:\n",
    "        y = df[label].values + 1 # Sumar uno a las labels para que [-1,0,1] => [0,1,2]\n",
    "    elif label in ['emotion', 'emotion4']:\n",
    "        emotion_mapping = {'Alegría': 0,'Ira': 1,'Neutral': 2,'Sorpresa': 3,'Asco': 4,'Temor': 5,'Tristeza': 6}\n",
    "        df[label] = df[label].map(emotion_mapping)\n",
    "        y = df[label].values\n",
    "    elif label == 'categories':\n",
    "        emotion_mapping = {'hap': 0,'ang': 1,'neu': 2,'sad': 3}\n",
    "        df[label] = df[label].map(emotion_mapping)\n",
    "        y = df[label].values \n",
    "    else: # emotion_code\n",
    "        y = df[label].values \n",
    "    groups = df['sub_id'].values\n",
    "    return X, y, groups\n",
    "\n",
    "def pipeline_classic_models(database, labels, path_main = 'results/', save = False):\n",
    "    set_seed(seed_value=13)\n",
    "    tic = time.time()\n",
    "    results = []\n",
    "    for prediction in labels:\n",
    "        if save:\n",
    "            path = path_main +database+'/classicalSVM_FeatLong/'\n",
    "            path_save = path_main +database+'/classicalSVM_FeatLong/'+database+'_classicalSVM_FeatLong_'+prediction+'.csv'\n",
    "            \n",
    "            field_names = ['label','% feat','resample','classifier','params','n-feat','features',\n",
    "                           'shape train','shape sel','shape test',\n",
    "                           'cv_mean_WAcc','cv_std_WAcc', 'cv_mean_UAcc','cv_std_UAcc','cv_mean_kappa','cv_std_kappa',\n",
    "                           'cv_mean_auc','cv_std_auc', 'cv_mean_f1','cv_std_f1',\n",
    "                           'cv_mean_precision','cv_std_precision', 'cv_mean_recall','cv_std_recall',\n",
    "#                            'cv_mean_sensitivity','cv_std_sensitivity','cv_mean_specificity','cv_std_specificity',\n",
    "                           'test_WAcc', 'test_UAcc', 'test_kappa', 'test_auc', 'test_f1', 'test_precision', 'test_recall',\n",
    "#                            'test_sensitivity','test_specificity',\n",
    "                           'test_cm']\n",
    "            \n",
    "            with open(path_save, 'w', newline='') as file:\n",
    "                dw = DictWriter(file, delimiter=',', fieldnames=field_names)\n",
    "                dw.writeheader()\n",
    "        \n",
    "        for percent_features in [0.25, 0.5, 0.75]:\n",
    "            print('[INFO] Loading data.. (% features keep =',percent_features,') ',prediction)\n",
    "            \n",
    "            # Load data\n",
    "            df = load_dataframe(database)\n",
    "            \n",
    "            # Eliminar audios con desacuerdo (label=10)\n",
    "            if database == 'IEMOCAP' and prediction == 'categories':\n",
    "                print('---------------- Convertir en int el 10!!')\n",
    "                df[prediction] = df[prediction].replace(['10'], 10)\n",
    "            df = df.loc[df[prediction]!=10].copy() \n",
    "            df.reset_index(drop=True,inplace=True)\n",
    "            \n",
    "            # Separate partitions\n",
    "            df_test = df.loc[df.split == 'test'].copy()\n",
    "            df_test.reset_index(inplace=True, drop = True)\n",
    "            df_develop = df.loc[df.split != 'test'].copy()\n",
    "            df_develop.reset_index(inplace=True, drop = True)\n",
    "            \n",
    "            # Obtain X,y,groups\n",
    "            X_develop, y_develop, groups_develop = separate_Xy(df_develop, prediction)\n",
    "            X_test, y_test, groups_test = separate_Xy(df_test, prediction)\n",
    "            print('Development:',X_develop.shape, Counter(y_develop))\n",
    "            print('Test:',X_test.shape, Counter(y_test))\n",
    "\n",
    "            # Feature normalization\n",
    "            scalar = StandardScaler()\n",
    "            scalar.fit(X_develop)\n",
    "            X_develop_scaled = pd.DataFrame(scalar.transform(X_develop), columns = X_develop.columns.values)\n",
    "            X_test_scaled = pd.DataFrame(scalar.transform(X_test), columns = X_test.columns.values)\n",
    "            \n",
    "            # Remove high correlated features\n",
    "            correlated_features = set()\n",
    "            correlation_matrix = X_develop_scaled.corr()\n",
    "            for i in range(len(correlation_matrix.columns)):\n",
    "                for j in range(i):\n",
    "                    if abs(correlation_matrix.iloc[i, j]) > 0.95:\n",
    "                        colname = correlation_matrix.columns[i]\n",
    "                        correlated_features.add(colname)\n",
    "            X_develop_scaled.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "            X_test_scaled.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "\n",
    "            # Feature selection\n",
    "            feat_sel = int(percent_features*X_develop_scaled.shape[1])\n",
    "            selectK = SelectKBest(f_classif, k=feat_sel)\n",
    "            selectK.fit(X_develop_scaled, y_develop)\n",
    "            cols = selectK.get_support(indices=True)\n",
    "            X_develop_scaled = X_develop_scaled.iloc[:,cols]\n",
    "            X_test_scaled =  X_test_scaled.iloc[:,cols]\n",
    "            selected_features = X_develop_scaled.columns.values\n",
    "            \n",
    "            X_develop_scaled = np.array(X_develop_scaled)\n",
    "            X_test_scaled = np.array(X_test_scaled)\n",
    "            \n",
    "            if database == 'IEMOCAP':\n",
    "                cv = StratifiedGroupKFold(n_splits=4, random_state=13, shuffle=True)\n",
    "            else:\n",
    "                cv = StratifiedGroupKFold(n_splits=5, random_state=13, shuffle=True) \n",
    "            for classifier in dict_classifiers:\n",
    "                for resample in ['None']: # + list(dict_resample.keys()): \n",
    "\n",
    "                    if resample == 'None':\n",
    "                        params = transform_dict(dict_parameters[classifier])\n",
    "                        grid_search = GridSearchCV(estimator=dict_classifiers[classifier],\n",
    "                                                   param_grid=params,\n",
    "                                                   scoring=scoring,\n",
    "                                                   cv=cv,\n",
    "                                                   refit='auc')\n",
    "                    else:\n",
    "                        pipeline = Pipeline(steps = [[resample, dict_resample[resample]],\n",
    "                                                     [classifier, dict_classifiers[classifier]]])\n",
    "\n",
    "                        grid_search = GridSearchCV(estimator=pipeline,\n",
    "                                                   param_grid=dict_parameters[classifier],\n",
    "                                                   scoring=scoring,\n",
    "                                                   cv=cv,\n",
    "                                                   refit='auc', error_score=\"raise\")\n",
    "                    grid_search.fit(X=X_develop_scaled, y=y_develop, groups=groups_develop)\n",
    "                    \n",
    "                    cv_mean_WAcc = grid_search.cv_results_['mean_test_WAcc'][grid_search.best_index_]\n",
    "                    cv_std_WAcc = grid_search.cv_results_['std_test_WAcc'][grid_search.best_index_]\n",
    "                    cv_mean_UAcc = grid_search.cv_results_['mean_test_UAcc'][grid_search.best_index_]\n",
    "                    cv_std_UAcc = grid_search.cv_results_['std_test_UAcc'][grid_search.best_index_]\n",
    "                    cv_mean_kappa = grid_search.cv_results_['mean_test_kappa'][grid_search.best_index_]\n",
    "                    cv_std_kappa = grid_search.cv_results_['std_test_kappa'][grid_search.best_index_]\n",
    "                    cv_mean_auc = grid_search.cv_results_['mean_test_auc'][grid_search.best_index_]\n",
    "                    cv_std_auc = grid_search.cv_results_['std_test_auc'][grid_search.best_index_]\n",
    "                    cv_mean_f1 = grid_search.cv_results_['mean_test_f1'][grid_search.best_index_]\n",
    "                    cv_std_f1 = grid_search.cv_results_['std_test_f1'][grid_search.best_index_]\n",
    "                    cv_mean_precision = grid_search.cv_results_['mean_test_precision'][grid_search.best_index_]\n",
    "                    cv_std_precision = grid_search.cv_results_['std_test_precision'][grid_search.best_index_]\n",
    "                    cv_mean_recall = grid_search.cv_results_['mean_test_recall'][grid_search.best_index_]\n",
    "                    cv_std_recall = grid_search.cv_results_['std_test_recall'][grid_search.best_index_]\n",
    "#                     cv_mean_sensitivity = grid_search.cv_results_['mean_test_sensitivity'][grid_search.best_index_]\n",
    "#                     cv_std_sensitivity = grid_search.cv_results_['std_test_sensitivity'][grid_search.best_index_]\n",
    "#                     cv_mean_specificity = grid_search.cv_results_['mean_test_specificity'][grid_search.best_index_]\n",
    "#                     cv_std_specificity = grid_search.cv_results_['std_test_specificity'][grid_search.best_index_]\n",
    "    \n",
    "                    if resample == 'None':\n",
    "                        dict_params = grid_search.best_params_\n",
    "                    else:\n",
    "                        dict_params = transform_dict(grid_search.best_params_)\n",
    "\n",
    "                    if classifier == 'SVC':\n",
    "                        clf_test = SVC(**dict_params)\n",
    "                    elif classifier == 'KNN':\n",
    "                        clf_test = KNN(**dict_params)\n",
    "                    elif classifier == 'LogReg':\n",
    "                        clf_test = LogisticRegression(**dict_params)\n",
    "\n",
    "                    clf_test.fit(X_develop_scaled, y_develop) \n",
    "                    y_true, y_pred = y_test, clf_test.predict(X_test_scaled)\n",
    "                    y_pred_prob = clf_test.predict_proba(X_test_scaled)\n",
    "                    \n",
    "                    test_WAcc = accuracy_score(y_true, y_pred)\n",
    "                    test_UAcc = balanced_accuracy_score(y_true, y_pred)\n",
    "                    test_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "                    test_auc = roc_auc_score(y_true, y_pred_prob, multi_class = 'ovr')\n",
    "                    test_f1 = f1_score(y_true, y_pred, average = 'weighted')\n",
    "                    test_precision = precision_score(y_true, y_pred, average = 'weighted')\n",
    "                    test_recall = recall_score(y_true, y_pred, average = 'weighted')\n",
    "#                     sensitivity = recall_score(y_true,y_pred,pos_label=1)\n",
    "#                     specificity = recall_score(y_true,y_pred,pos_label=0)    \n",
    "                    cm_test = confusion_matrix(y_true, y_pred)\n",
    "                    cm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_string = ''\n",
    "                    for i in range(len(cm_test)):\n",
    "                        cm_string = cm_string + str(cm_test[i]) + ' '\n",
    "    \n",
    "                    row = {'label':prediction,\n",
    "                           '% feat': percent_features,\n",
    "                           'resample': resample,\n",
    "                           'classifier':classifier,\n",
    "                           'params':grid_search.best_params_,\n",
    "                           'n-feat': feat_sel,'features':selected_features,\n",
    "                           'shape train':X_develop.shape,'shape sel':X_develop_scaled.shape,'shape test':X_test_scaled.shape,\n",
    "                           'cv_mean_WAcc':cv_mean_WAcc,'cv_std_WAcc':cv_std_WAcc,\n",
    "                           'cv_mean_UAcc':cv_mean_UAcc,'cv_std_UAcc':cv_std_UAcc,\n",
    "                           'cv_mean_kappa':cv_mean_kappa,'cv_std_kappa':cv_std_kappa,\n",
    "                           'cv_mean_auc':cv_mean_auc,'cv_std_auc':cv_std_auc,\n",
    "                           'cv_mean_f1':cv_mean_f1,'cv_std_f1':cv_std_f1,\n",
    "                           'cv_mean_precision':cv_mean_precision,'cv_std_precision':cv_std_precision,\n",
    "                           'cv_mean_recall':cv_mean_recall,'cv_std_recall':cv_std_recall,\n",
    "#                            'cv_mean_sensitivity':cv_mean_sensitivity,'cv_std_sensitivity':cv_std_sensitivity,\n",
    "#                            'cv_mean_specificity':cv_mean_specificity,'cv_std_specificity':cv_std_specificity,\n",
    "                           'test_WAcc':test_WAcc,\n",
    "                           'test_UAcc':test_UAcc,\n",
    "                           'test_kappa':test_kappa,\n",
    "                           'test_auc':test_auc,\n",
    "                           'test_f1':test_f1,\n",
    "                           'test_precision':test_precision,\n",
    "                           'test_recall':test_recall,\n",
    "#                            'test_sensitivity':test_sensitivity\n",
    "#                            'test_specificity':test_specificity\n",
    "                           'test_cm': cm_string}\n",
    "                    results.append(row)\n",
    "                    if save:\n",
    "                        with open(path_save, 'a', newline='') as f_object:\n",
    "                            dictwriter_object = DictWriter(f_object, fieldnames=list(row.keys()))\n",
    "                            dictwriter_object.writerow(row)\n",
    "                            f_object.close()\n",
    "                            \n",
    "                        # Save groups_test y_true, y_pred del test\n",
    "                        folder = path + prediction + '/'\n",
    "                        makedir(folder)\n",
    "                        df_predictions = pd.DataFrame({'file': df_test['file'].values,\n",
    "                                                        'subject': groups_test,\n",
    "                                                        'y_true': y_true,\n",
    "                                                        'y_pred': y_pred})\n",
    "                        df_predictions.to_csv(folder + 'feat'+str(percent_features)+'_resample'+str(resample)+'_classsifier'+classifier+'.csv',index=False)\n",
    "                    \n",
    "            print('Done!')\n",
    "\n",
    "    toc = time.time()\n",
    "    print('Duration:',round(toc-tic,2)/60,'min')\n",
    "    \n",
    "    df_resultados_concat = pd.DataFrame.from_records(results)\n",
    "    \n",
    "    return df_resultados_concat\n",
    "\n",
    "def set_seed(seed_value=13):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "#     torch.manual_seed(seed_value)\n",
    "#     torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def makedir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data.. (% features keep = 0.25 )  valence\n",
      "Development: (1140, 88) Counter({0: 608, 2: 304, 1: 228})\n",
      "Test: (300, 88) Counter({0: 160, 2: 80, 1: 60})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  valence\n",
      "Development: (1140, 88) Counter({0: 608, 2: 304, 1: 228})\n",
      "Test: (300, 88) Counter({0: 160, 2: 80, 1: 60})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  valence\n",
      "Development: (1140, 88) Counter({0: 608, 2: 304, 1: 228})\n",
      "Test: (300, 88) Counter({0: 160, 2: 80, 1: 60})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.25 )  arousal\n",
      "Development: (1140, 88) Counter({2: 608, 0: 304, 1: 228})\n",
      "Test: (300, 88) Counter({2: 160, 0: 80, 1: 60})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  arousal\n",
      "Development: (1140, 88) Counter({2: 608, 0: 304, 1: 228})\n",
      "Test: (300, 88) Counter({2: 160, 0: 80, 1: 60})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  arousal\n",
      "Development: (1140, 88) Counter({2: 608, 0: 304, 1: 228})\n",
      "Test: (300, 88) Counter({2: 160, 0: 80, 1: 60})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.25 )  emotion_code\n",
      "Development: (1140, 88) Counter({4: 152, 6: 152, 5: 152, 7: 152, 2: 152, 3: 152, 8: 152, 1: 76})\n",
      "Test: (300, 88) Counter({2: 40, 5: 40, 7: 40, 8: 40, 3: 40, 4: 40, 6: 40, 1: 20})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  emotion_code\n",
      "Development: (1140, 88) Counter({4: 152, 6: 152, 5: 152, 7: 152, 2: 152, 3: 152, 8: 152, 1: 76})\n",
      "Test: (300, 88) Counter({2: 40, 5: 40, 7: 40, 8: 40, 3: 40, 4: 40, 6: 40, 1: 20})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  emotion_code\n",
      "Development: (1140, 88) Counter({4: 152, 6: 152, 5: 152, 7: 152, 2: 152, 3: 152, 8: 152, 1: 76})\n",
      "Test: (300, 88) Counter({2: 40, 5: 40, 7: 40, 8: 40, 3: 40, 4: 40, 6: 40, 1: 20})\n",
      "Done!\n",
      "Duration: 13.824166666666667 min\n"
     ]
    }
   ],
   "source": [
    "database = 'RAVDESS'\n",
    "labels = ['valence','arousal','emotion_code']\n",
    "df_resultados = pipeline_classic_models(database, labels, path_main = 'results_final/', save = True)\n",
    "# Duration: 15 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOSOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data.. (% features keep = 0.25 )  valence_3r\n",
      "Development: (801, 88) Counter({1: 363, 2: 219, 0: 219})\n",
      "Test: (192, 88) Counter({1: 70, 2: 66, 0: 56})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  valence_3r\n",
      "Development: (801, 88) Counter({1: 363, 2: 219, 0: 219})\n",
      "Test: (192, 88) Counter({1: 70, 2: 66, 0: 56})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  valence_3r\n",
      "Development: (801, 88) Counter({1: 363, 2: 219, 0: 219})\n",
      "Test: (192, 88) Counter({1: 70, 2: 66, 0: 56})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.25 )  arousal_3r\n",
      "Development: (731, 88) Counter({2: 411, 1: 244, 0: 76})\n",
      "Test: (192, 88) Counter({2: 128, 1: 51, 0: 13})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lugoza\\.conda\\envs\\voice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  arousal_3r\n",
      "Development: (731, 88) Counter({2: 411, 1: 244, 0: 76})\n",
      "Test: (192, 88) Counter({2: 128, 1: 51, 0: 13})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  arousal_3r\n",
      "Development: (731, 88) Counter({2: 411, 1: 244, 0: 76})\n",
      "Test: (192, 88) Counter({2: 128, 1: 51, 0: 13})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lugoza\\.conda\\envs\\voice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.25 )  valence_psy\n",
      "Development: (807, 88) Counter({2: 309, 0: 251, 1: 247})\n",
      "Test: (192, 88) Counter({2: 87, 0: 59, 1: 46})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  valence_psy\n",
      "Development: (807, 88) Counter({2: 309, 0: 251, 1: 247})\n",
      "Test: (192, 88) Counter({2: 87, 0: 59, 1: 46})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  valence_psy\n",
      "Development: (807, 88) Counter({2: 309, 0: 251, 1: 247})\n",
      "Test: (192, 88) Counter({2: 87, 0: 59, 1: 46})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.25 )  arousal_psy\n",
      "Development: (807, 88) Counter({1: 347, 0: 265, 2: 195})\n",
      "Test: (192, 88) Counter({2: 82, 1: 76, 0: 34})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lugoza\\.conda\\envs\\voice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  arousal_psy\n",
      "Development: (807, 88) Counter({1: 347, 0: 265, 2: 195})\n",
      "Test: (192, 88) Counter({2: 82, 1: 76, 0: 34})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  arousal_psy\n",
      "Development: (807, 88) Counter({1: 347, 0: 265, 2: 195})\n",
      "Test: (192, 88) Counter({2: 82, 1: 76, 0: 34})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lugoza\\.conda\\envs\\voice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.25 )  valencia_combined_v2\n",
      "Development: (807, 88) Counter({1: 305, 2: 261, 0: 241})\n",
      "Test: (192, 88) Counter({2: 73, 1: 62, 0: 57})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  valencia_combined_v2\n",
      "Development: (807, 88) Counter({1: 305, 2: 261, 0: 241})\n",
      "Test: (192, 88) Counter({2: 73, 1: 62, 0: 57})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  valencia_combined_v2\n",
      "Development: (807, 88) Counter({1: 305, 2: 261, 0: 241})\n",
      "Test: (192, 88) Counter({2: 73, 1: 62, 0: 57})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.25 )  arousal_combined_v2\n",
      "Development: (807, 88) Counter({2: 331, 1: 328, 0: 148})\n",
      "Test: (192, 88) Counter({2: 109, 1: 70, 0: 13})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  arousal_combined_v2\n",
      "Development: (807, 88) Counter({2: 331, 1: 328, 0: 148})\n",
      "Test: (192, 88) Counter({2: 109, 1: 70, 0: 13})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  arousal_combined_v2\n",
      "Development: (807, 88) Counter({2: 331, 1: 328, 0: 148})\n",
      "Test: (192, 88) Counter({2: 109, 1: 70, 0: 13})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.25 )  emotion4\n",
      "Development: (710, 88) Counter({0: 264, 2: 184, 1: 157, 3: 105})\n",
      "Test: (174, 88) Counter({0: 78, 1: 42, 2: 41, 3: 13})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lugoza\\.conda\\envs\\voice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  emotion4\n",
      "Development: (710, 88) Counter({0: 264, 2: 184, 1: 157, 3: 105})\n",
      "Test: (174, 88) Counter({0: 78, 1: 42, 2: 41, 3: 13})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  emotion4\n",
      "Development: (710, 88) Counter({0: 264, 2: 184, 1: 157, 3: 105})\n",
      "Test: (174, 88) Counter({0: 78, 1: 42, 2: 41, 3: 13})\n",
      "Done!\n",
      "Duration: 22.943166666666666 min\n"
     ]
    }
   ],
   "source": [
    "database = 'VOSOME'\n",
    "labels = ['valence_3r', 'arousal_3r', \n",
    "          'valence_psy', 'arousal_psy',\n",
    "          'valencia_combined_v2', 'arousal_combined_v2',\n",
    "          'emotion4']\n",
    "df_resultados = pipeline_classic_models(database, labels, path_main = 'results_final/', save = True)\n",
    "# Duration: 23 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEMOCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data.. (% features keep = 0.25 )  categories\n",
      "---------------- Convertir en int el 10!!\n",
      "Development: (4290, 88) Counter({2: 1324, 0: 1194, 1: 933, 3: 839})\n",
      "Test: (1241, 88) Counter({0: 442, 2: 384, 3: 245, 1: 170})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.5 )  categories\n",
      "---------------- Convertir en int el 10!!\n",
      "Development: (4290, 88) Counter({2: 1324, 0: 1194, 1: 933, 3: 839})\n",
      "Test: (1241, 88) Counter({0: 442, 2: 384, 3: 245, 1: 170})\n",
      "Done!\n",
      "[INFO] Loading data.. (% features keep = 0.75 )  categories\n",
      "---------------- Convertir en int el 10!!\n",
      "Development: (4290, 88) Counter({2: 1324, 0: 1194, 1: 933, 3: 839})\n",
      "Test: (1241, 88) Counter({0: 442, 2: 384, 3: 245, 1: 170})\n",
      "Done!\n",
      "Duration: 43.339000000000006 min\n"
     ]
    }
   ],
   "source": [
    "database = 'IEMOCAP'\n",
    "labels = ['categories'] # 'valence','arousal', \n",
    "\n",
    "df_resultados = pipeline_classic_models(database, labels, path_main = 'results_final/', save = True)\n",
    "# Duration = 3h valence, 2.5h arousal, 45min categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 84.767,
   "position": {
    "height": "324px",
    "left": "1617px",
    "right": "20px",
    "top": "117px",
    "width": "272px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
