{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8759a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install -r requirements_huggingface.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ad879",
   "metadata": {},
   "source": [
    "# Pre-trained models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b8700",
   "metadata": {},
   "source": [
    "| Pre-trained model | Name in the article | URL |\n",
    "| :- | :- | :- |\n",
    "| facebook/wav2vec2-xls-r-300m | w2v2-xlsr-128 | https://huggingface.co/facebook/wav2vec2-xls-r-300m |\n",
    "| facebook/wav2vec2-large-xlsr-53 | w2v2-xlsr-53 | https://huggingface.co/facebook/wav2vec2-large-xlsr-53 |\n",
    "| facebook/wav2vec2-large-xlsr-53-spanish | w2v2-xlsr-53-spa | https://huggingface.co/facebook/wav2vec2-large-xlsr-53-spanish |\n",
    "| facebook/wav2vec2-large-robust | w2v2-L-robust | https://huggingface.co/facebook/wav2vec2-large-robust |\n",
    "| facebook/hubert-large-ll60k | hubert-L | https://huggingface.co/facebook/hubert-large-ll60k |\n",
    "| Microsoft's UniSpeech-SAT-Large | unispeech-L | https://huggingface.co/microsoft/unispeech-sat-large |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfeb9e",
   "metadata": {},
   "source": [
    "# 1. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8670d012",
   "metadata": {},
   "source": [
    "## 1.1 Calculate embedding for an audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46eda8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2Model: ['quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_q.weight', 'project_hid.bias', 'quantizer.codevectors']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 164, 1024)\n"
     ]
    }
   ],
   "source": [
    "import torch  # Import the PyTorch library for tensor computations\n",
    "import numpy as np  # Import the NumPy library for numerical operations\n",
    "import librosa  # Import the Librosa library for audio processing\n",
    "import os  # Import the OS library for interacting with the file system\n",
    "import time  # Import the Time library for timing operations\n",
    "from transformers import AutoFeatureExtractor, AutoModel  # Import Hugging Face's transformers for feature extraction and model loading\n",
    "\n",
    "# Audio example\n",
    "path = 'D:/lugoza/Databases/RAVDESS/audios/Actor_01/03-01-01-01-01-01-01.wav'\n",
    "\n",
    "# Model to be used\n",
    "model_name = \"facebook/wav2vec2-xls-r-300m\"\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the audio file\n",
    "input_audio, sample_rate = librosa.load(path, sr = 16000)\n",
    "# duration = librosa.get_duration(y=input_audio, sr=sample_rate)\n",
    "\n",
    "# Load pre-trained model\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Extract audio features and move to the appropriate device\n",
    "audio_embeddings = feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate).to(device)\n",
    "\n",
    "# Perform inference without computing gradients\n",
    "with torch.no_grad():\n",
    "    outputs = model(audio_embeddings.input_values)\n",
    "                \n",
    "# Detach the output and move to CPU if necessary, then convert to NumPy array\n",
    "if device == 'cpu':\n",
    "    x = outputs.last_hidden_state.detach().numpy()\n",
    "else:\n",
    "    x = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "                    \n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e41b36",
   "metadata": {},
   "source": [
    "## 1.2 Load embeddings and plot dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02970e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/wav2vec2-xls-r-300m : (1, 164, 1024)\n",
      "facebook/wav2vec2-large-xlsr-53 : (1, 164, 1024)\n",
      "facebook/wav2vec2-large-xlsr-53-spanish : (1, 164, 1024)\n",
      "facebook/wav2vec2-large-robust : (1, 164, 1024)\n",
      "facebook/hubert-large-ll60k : (1, 164, 1024)\n",
      "microsoft/unispeech-sat-large : (1, 164, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Path to the embeddings directory\n",
    "path_embeddings = 'D:/lugoza/Databases/RAVDESS/embeddings/'\n",
    "example_file = '03-01-01-01-01-01-01.npy'\n",
    "\n",
    "# List of model IDs used\n",
    "model_ids = [\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    \"facebook/wav2vec2-large-xlsr-53\",\n",
    "    \"facebook/wav2vec2-large-xlsr-53-spanish\",\n",
    "    \"facebook/wav2vec2-large-robust\",\n",
    "    \"facebook/hubert-large-ll60k\",\n",
    "    \"microsoft/unispeech-sat-large\",\n",
    "]\n",
    "\n",
    "# Loop through each model\n",
    "for model_name in model_ids:\n",
    "    # Define the path to save embeddings\n",
    "    path = path_embeddings + model_name.replace(\"/\", \"-\") + '/audio_embeddings/'\n",
    "    \n",
    "    # Load the embeddings from the specified file\n",
    "    emb = np.load(path + example_file)\n",
    "    \n",
    "    # Print the shape of the loaded embeddings\n",
    "    print(model_name, ':', emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965c078",
   "metadata": {},
   "source": [
    "# 2. Extract embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208cece",
   "metadata": {},
   "source": [
    "The code used to extract embeddings is consistent across all the databases studied: EMOVOME, RAVDESS, and IEMOCAP. \n",
    "\n",
    "The only variations are due to the different file structures of each database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f029a",
   "metadata": {},
   "source": [
    "## 2.1 EMOVOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf39a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Import the PyTorch library for tensor computations\n",
    "import numpy as np  # Import the NumPy library for numerical operations\n",
    "import librosa  # Import the Librosa library for audio processing\n",
    "import os  # Import the OS library for interacting with the file system\n",
    "import time  # Import the Time library for timing operations\n",
    "from transformers import AutoFeatureExtractor, AutoModel  # Import Hugging Face's transformers for feature extraction and model loading\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# List of model IDs to be used\n",
    "model_ids = [\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    \"facebook/wav2vec2-large-xlsr-53\",\n",
    "    \"facebook/wav2vec2-large-xlsr-53-spanish\",\n",
    "    \"facebook/wav2vec2-large-robust\",\n",
    "    \"facebook/hubert-large-ll60k\",\n",
    "    \"microsoft/unispeech-sat-large\",\n",
    "]\n",
    "\n",
    "# Define the database and path to audio files\n",
    "database = 'EMOVOME' \n",
    "path = 'data/' + database + '/audios'\n",
    "\n",
    "# Loop through each model\n",
    "for model_name in model_ids:\n",
    "    tic = time.time()  # Start timing\n",
    "\n",
    "    # Define the path to save embeddings\n",
    "    path_save = 'data/' + database + '/embeddings/' + model_name.replace(\"/\", \"-\") + '/audio_embeddings/'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "        \n",
    "    # Load pre-trained models\n",
    "    print('MODEL: ', model_name)\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Calculate embeddings for each audio file\n",
    "    for audio in os.listdir(path):\n",
    "        # Load the audio file\n",
    "        input_audio, sample_rate = librosa.load(path + '/' + audio, sr=16000)\n",
    "            # It is possible to cut the audio to a specific duration using the duration parameter as follows:\n",
    "            # input_audio, sample_rate = librosa.load(path+'/'+audio, sr = 16000, duration = 30)\n",
    "            \n",
    "        # Extract audio features and move to the appropriate device\n",
    "        audio_embeddings = feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate).to(device)\n",
    "        \n",
    "        # Perform inference without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(audio_embeddings.input_values)\n",
    "        \n",
    "        # Detach the output and move to CPU if necessary, then convert to NumPy array\n",
    "        if device == 'cpu':\n",
    "            x = outputs.last_hidden_state.detach().numpy()\n",
    "        else:\n",
    "            x = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "        \n",
    "        # Save the embeddings to a file\n",
    "        id_audio = audio.split('.')[0]  # Extract the base name of the audio file\n",
    "        with open(path_save + id_audio + '.npy', 'wb') as f:\n",
    "            np.save(f, x)\n",
    "    \n",
    "    toc = time.time()  # End timing\n",
    "    print(model_name, ' - Duration:', round((toc - tic) / 60, 2), 'min')  # Print the duration of processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f09097",
   "metadata": {},
   "source": [
    "## 2.2 RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Import the PyTorch library for tensor computations\n",
    "import numpy as np  # Import the NumPy library for numerical operations\n",
    "import librosa  # Import the Librosa library for audio processing\n",
    "import os  # Import the OS library for interacting with the file system\n",
    "import time  # Import the Time library for timing operations\n",
    "from transformers import AutoFeatureExtractor, AutoModel  # Import Hugging Face's transformers for feature extraction and model loading\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# List of model IDs to be used\n",
    "model_ids = [\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    \"facebook/wav2vec2-large-xlsr-53\",\n",
    "    \"facebook/wav2vec2-large-xlsr-53-spanish\",\n",
    "    \"facebook/wav2vec2-large-robust\",\n",
    "    \"facebook/hubert-large-ll60k\",\n",
    "    \"microsoft/unispeech-sat-large\",\n",
    "]\n",
    "\n",
    "# Define the database and path to audio files\n",
    "database = 'RAVDESS'\n",
    "path = 'data/' + database + '/audios'\n",
    "\n",
    "# Loop through each model\n",
    "for model_name in model_ids:\n",
    "    tic = time.time()  # Start timing\n",
    "\n",
    "    # Define the path to save embeddings\n",
    "    path_save = 'data/' + database + '/embeddings/' + model_name.replace(\"/\", \"-\") + '/audio_embeddings/'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "        \n",
    "    # Load pre-trained models\n",
    "    print('MODEL: ', model_name)\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Calculate embeddings for each audio file\n",
    "    for folder in os.listdir(path):\n",
    "        for audio in os.listdir(path+'/'+folder+'/'):\n",
    "            # Load the audio file\n",
    "            input_audio, sample_rate = librosa.load(path+'/'+folder+'/'+audio, sr=16000)\n",
    "                # It is possible to cut the audio to a specific duration using the duration parameter as follows:\n",
    "                # input_audio, sample_rate = librosa.load(path+'/'+audio, sr = 16000, duration = 30)\n",
    "\n",
    "            # Extract audio features and move to the appropriate device\n",
    "            audio_embeddings = feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate).to(device)\n",
    "\n",
    "            # Perform inference without computing gradients\n",
    "            with torch.no_grad():\n",
    "                outputs = model(audio_embeddings.input_values)\n",
    "\n",
    "            # Detach the output and move to CPU if necessary, then convert to NumPy array\n",
    "            if device == 'cpu':\n",
    "                x = outputs.last_hidden_state.detach().numpy()\n",
    "            else:\n",
    "                x = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "\n",
    "            # Save the embeddings to a file\n",
    "            id_audio = audio.split('.')[0]  # Extract the base name of the audio file\n",
    "            with open(path_save + id_audio + '.npy', 'wb') as f:\n",
    "                np.save(f, x)\n",
    "    \n",
    "    toc = time.time()  # End timing\n",
    "    print(model_name, ' - Duration:', round((toc - tic) / 60, 2), 'min')  # Print the duration of processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14e090",
   "metadata": {},
   "source": [
    "## 2.3 IEMOCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Import the PyTorch library for tensor computations\n",
    "import numpy as np  # Import the NumPy library for numerical operations\n",
    "import librosa  # Import the Librosa library for audio processing\n",
    "import os  # Import the OS library for interacting with the file system\n",
    "import time  # Import the Time library for timing operations\n",
    "from transformers import AutoFeatureExtractor, AutoModel  # Import Hugging Face's transformers for feature extraction and model loading\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# List of model IDs to be used\n",
    "model_ids = [\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    \"facebook/wav2vec2-large-xlsr-53\",\n",
    "    \"facebook/wav2vec2-large-xlsr-53-spanish\",\n",
    "    \"facebook/wav2vec2-large-robust\",\n",
    "    \"facebook/hubert-large-ll60k\",\n",
    "    \"microsoft/unispeech-sat-large\",\n",
    "]\n",
    "\n",
    "# Define the database and path to audio files\n",
    "database = 'RAVDESS'\n",
    "path = 'data/' + database + '/audios'\n",
    "\n",
    "# Loop through each model\n",
    "for model_name in model_ids:\n",
    "    tic = time.time()  # Start timing\n",
    "\n",
    "    # Define the path to save embeddings\n",
    "    path_save = 'data/' + database + '/embeddings/' + model_name.replace(\"/\", \"-\") + '/audio_embeddings/'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "        \n",
    "    # Load pre-trained models\n",
    "    print('MODEL: ', model_name)\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Calculate embeddings for each audio file\n",
    "    for session in [folder for folder in os.listdir(path) if folder.startswith('Session')]:\n",
    "        for improv in os.listdir(path + session +'/sentences/wav/'):\n",
    "            for file in [f for f in os.listdir(path + session +'/sentences/wav/'+improv+'/') if f.endswith('.wav')]:\n",
    "                \n",
    "                path_file = path + session +'/sentences/wav/'+improv+'/'+file    \n",
    "                \n",
    "                # Load the audio file\n",
    "                input_audio, sample_rate = librosa.load(path_file, sr = 16000)\n",
    "                    # It is possible to cut the audio to a specific duration using the duration parameter as follows:\n",
    "                    # input_audio, sample_rate = librosa.load(path+'/'+audio, sr = 16000, duration = 30)\n",
    "                \n",
    "                # Extract audio features and move to the appropriate device\n",
    "                audio_embeddings = feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate).to(device)\n",
    "                \n",
    "                # Perform inference without computing gradients\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(audio_embeddings.input_values)\n",
    "                    \n",
    "                # Detach the output and move to CPU if necessary, then convert to NumPy array\n",
    "                if device == 'cpu':\n",
    "                    x = outputs.last_hidden_state.detach().numpy()\n",
    "                else:\n",
    "                    x = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "                    \n",
    "                # Save the embeddings to a file\n",
    "                id_audio = file.split('.')[0]  # Extract the base name of the audio file\n",
    "                with open(path_save+id_audio+'.npy', 'wb') as f:\n",
    "                    np.save(f, x)\n",
    "                    \n",
    "    toc = time.time()  # End timing\n",
    "    print(model_name, ' - Duration:', round((toc - tic) / 60, 2), 'min')  # Print the duration of processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
